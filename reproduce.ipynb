{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparison of Metric Learning Loss Functions for End-to-End Speaker Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to reproduce the equal error rate of the additive angular margin loss model from the paper.\n",
    "\n",
    "Before you begin, make sure you have installed [pyannote-audio](https://github.com/pyannote/pyannote-audio) and [pyannote.db.voxceleb](https://github.com/pyannote/pyannote-db-voxceleb).\n",
    "\n",
    "If you use this model, please cite our paper:\n",
    "\n",
    "```BibTeX soon!```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we apply the pretrained model to generate `VoxCeleb1` embeddings. Audio chunk duration is 3s as indicated in `config.yml`, and the step of the sliding window is ~100ms (3 * 0.0333).\n",
    "\n",
    "To do so, you need to execute the following commands in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ pyannote-audio emb apply --gpu --step=0.0333 --batch=128 --subset=test models/AAM/train/VoxCeleb.SpeakerVerification.VoxCeleb2.train/validate_equal_error_rate/VoxCeleb.SpeakerVerification.VoxCeleb1_X.development VoxCeleb.SpeakerVerification.VoxCeleb1_X\n",
    "$ pyannote-audio emb apply --gpu --step=0.0333 --batch=128 --subset=train models/AAM/train/VoxCeleb.SpeakerVerification.VoxCeleb2.train/validate_equal_error_rate/VoxCeleb.SpeakerVerification.VoxCeleb1_X.development VoxCeleb.SpeakerVerification.VoxCeleb1_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you need to remove `--gpu` if your machine doesn't have one. These commands may take some time to execute, especially the second one. However, once all embeddings are calculated, you will be free to run this notebook many times and modify it without waiting for hours to obtain results.\n",
    "\n",
    "In this notebook we will be using the `Test` subset to calculate the EER, and `Train` to normalize similarity scores with adaptive s-norm.\n",
    "\n",
    "If you want to know more about the `apply` method, you can check out [pyannote's tutorials](https://github.com/pyannote/pyannote-audio/tree/develop/tutorials/models/speaker_embedding#application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all the needed libraries are installed:\n",
    "- `numpy` for obvious reasons\n",
    "- `xarray` to facilitate score normalization\n",
    "- `feerci` to calculate EER and its confidence interval\n",
    "- `pyannote.audio` to use pretrained models\n",
    "- `pyannote.database` to access `VoxCeleb1`\n",
    "- `tqdm` to show nice progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xarray import DataArray\n",
    "from feerci import feerci\n",
    "from pyannote.core.utils.distance import cdist\n",
    "from pyannote.audio.features import Precomputed\n",
    "from pyannote.audio.applications.speaker_embedding import SpeakerEmbedding\n",
    "from pyannote.database import get_protocol\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialize the database with the preprocessors needed, and we define some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the VoxCeleb1_X protocol, with a train and dev set\n",
    "# resulting from splitting the original development set\n",
    "protocol = get_protocol('VoxCeleb.SpeakerVerification.VoxCeleb1_X')\n",
    "\n",
    "\n",
    "# A function to crop the embeddings from a file\n",
    "def get_embedding(file, pretrained, mean=False):\n",
    "    emb = []\n",
    "    for f in file.files():\n",
    "        if 'try_with' in f:\n",
    "            segments = f['try_with']\n",
    "        else:\n",
    "            segments = f['annotation'].get_timeline()\n",
    "        for segment in segments:\n",
    "            for mode in ['center', 'loose']:\n",
    "                e = pretrained.crop(f, segment, mode=mode)\n",
    "                if len(e) > 0:\n",
    "                    break\n",
    "            emb.append(e)\n",
    "    emb = np.vstack(emb)\n",
    "    if mean:\n",
    "        emb = np.mean(emb, axis=0, keepdims=True)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# A function to calculate the EER on a subset of VoxCeleb1_X\n",
    "def run_experiment(distance, subset):\n",
    "    total = 37720 if subset == 'test' else None\n",
    "    y_pred, y_true = [], []\n",
    "    for trial in tqdm(getattr(protocol, f'{subset}_trial')(), total=total):\n",
    "        file1 = trial['file1']\n",
    "        hash1 = get_hash(file1)\n",
    "        file2 = trial['file2']\n",
    "        hash2 = get_hash(file2)\n",
    "        y_pred.append(distance.data[index1[hash1], index2[hash2]])\n",
    "        y_true.append(trial['reference'])\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    eer, ci_lower, ci_upper, _ = feerci(-y_pred[y_true == 0],\n",
    "                                        -y_pred[y_true == 1],\n",
    "                                        is_sorted=False)\n",
    "    return {\n",
    "        'eer': eer,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of 3s duration and of dimension 512, extracted every 99.9ms\n"
     ]
    }
   ],
   "source": [
    "# Load the precomputed embeddings calculated at the beginning of the notebook\n",
    "model = Precomputed(\n",
    "    'models/AAM/train/VoxCeleb.SpeakerVerification.VoxCeleb2.train/validate_equal_error_rate/'\n",
    "    'VoxCeleb.SpeakerVerification.VoxCeleb1_X.development/apply/0560/',\n",
    "    use_memmap=False)\n",
    "\n",
    "print(f'Embeddings of {model.sliding_window.duration:g}s duration and of dimension {model.dimension:d}, '\n",
    "      f'extracted every {1000 * model.sliding_window.step:g}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with Raw Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37720/37720 [00:29<00:00, 1282.89it/s]\n"
     ]
    }
   ],
   "source": [
    "get_hash = lambda file: SpeakerEmbedding.get_hash(file)\n",
    "\n",
    "# hash to embedding mapping\n",
    "cache1 = dict()\n",
    "cache2 = dict()\n",
    "\n",
    "# hash to index mapping\n",
    "index1 = dict()\n",
    "index2 = dict()\n",
    "\n",
    "n_file1 = 0\n",
    "n_file2 = 0\n",
    "\n",
    "# Get embeddings for every trial in Test\n",
    "for trial in tqdm(protocol.test_trial(), total=37720):\n",
    "    \n",
    "    file1 = trial['file1']\n",
    "    hash1 = get_hash(file1)\n",
    "    if hash1 not in cache1:\n",
    "        cache1[hash1] = get_embedding(file1, model, mean=True)\n",
    "        index1[hash1] = n_file1\n",
    "        n_file1 += 1\n",
    "    \n",
    "    file2 = trial['file2']\n",
    "    hash2 = get_hash(file2)\n",
    "    if hash2 not in cache2:\n",
    "        cache2[hash2] = get_embedding(file2, model, mean=True)\n",
    "        index2[hash2] = n_file2\n",
    "        n_file2 += 1\n",
    "\n",
    "hashes1 = list(cache1.keys())\n",
    "hashes2 = list(cache2.keys())\n",
    "emb1 = np.vstack(list(cache1.values()))\n",
    "emb2 = np.vstack(list(cache2.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cosine distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = DataArray(\n",
    "    cdist(emb1, emb2, metric='cosine'),\n",
    "    dims=('file1', 'file2'),\n",
    "    coords=(hashes1, hashes2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate EER on VoxCeleb1 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37720it [00:15, 2401.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER with raw distances: 3.94 in [3.75, 4.14]\n"
     ]
    }
   ],
   "source": [
    "raw_results = run_experiment(distance, 'test')\n",
    "print(f\"EER with raw distances: {100 * raw_results['eer']:.2f} in \"\n",
    "      f\"[{100 * raw_results['ci_lower']:.2f}, {100 * raw_results['ci_upper']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with adaptive s-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we improve the above EER with adaptive s-norm score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cohort set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cohort embeddings from VoxCeleb1_X.train\n",
    "cohort_embedding = dict()\n",
    "for cohort_file in tqdm(protocol.train(), total=143506):\n",
    "    speaker = cohort_file['annotation'].argmax()\n",
    "    embedding = get_embedding(cohort_file, model, mean=False)\n",
    "    cohort_embedding.setdefault(speaker, []).append(embedding)\n",
    "\n",
    "# The cohort consists of the mean embedding for each speaker\n",
    "cohort_speakers = list(cohort_embedding.keys())\n",
    "cohort = np.vstack([np.mean(np.vstack(cohort_embedding[speaker]), axis=0, keepdims=True) \n",
    "                    for speaker in cohort_speakers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate raw trial scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distances between each trial embedding (file1 and file2) and the cohort\n",
    "distance1 = DataArray(\n",
    "    cdist(emb1, cohort, metric='cosine'),\n",
    "    dims=('file1', 'cohort'),\n",
    "    coords=(hashes1, cohort_speakers))\n",
    "\n",
    "distance2 = DataArray(\n",
    "    cdist(emb2, cohort, metric='cosine'),\n",
    "    dims=('file2', 'cohort'),\n",
    "    coords=(hashes2, cohort_speakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize scores w.r.t the N most similar cohort embeddings\n",
    "\n",
    "N=400 for us. We have previously tuned this value on `VoxCeleb1 Dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our N\n",
    "COHORT_SIZE = 400\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for file1\n",
    "data1 = np.partition(distance1.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mz = np.mean(data1, axis=1) \n",
    "sz = np.std(data1, axis=1)\n",
    "mz = DataArray(mz, dims=('file1',), coords=(hashes1,))\n",
    "sz = DataArray(sz, dims=('file1',), coords=(hashes1,))\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for file2\n",
    "data2 = np.partition(distance2.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mt = np.mean(data2, axis=1) \n",
    "st = np.std(data2, axis=1)\n",
    "mt = DataArray(mt, dims=('file2',), coords=(hashes2,))\n",
    "st = DataArray(st, dims=('file2',), coords=(hashes2,))\n",
    "\n",
    "# Normalize\n",
    "distance_z = (distance - mz) / sz\n",
    "distance_t = (distance - mt) / st\n",
    "distance_s = 0.5 * (distance_z + distance_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate EER on VoxCeleb1 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the DET curve on test and print the EER value\n",
    "ada_snorm_results = run_experiment(distance_s, 'test')\n",
    "print(f\"EER with adaptive s-norm: {100 * ada_snorm_results['eer']:.2f} in\"\n",
    "      f\"[{100 * ada_snorm_results['ci_lower']:.2f}, {100 * ada_snorm_results['ci_upper']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! If you have any questions or suggestions, feel free to open an issue here or on [pyannote-audio](https://github.com/pyannote/pyannote-audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
