{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparison of Metric Learning Loss Functions for True End-to-End Speaker Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to reproduce the Equal Error Rate (EER) of the Additive Angular Margin (AAM) loss model from the paper.\n",
    "\n",
    "Before you begin, make sure you have installed both `pyannote.db.voxceleb` plugin [here](https://github.com/pyannote/pyannote-db-voxceleb).\n",
    "\n",
    "If you use this model, please cite our paper:\n",
    "\n",
    "```BibTeX Entry```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, we make sure all the needed libraries are available:\n",
    "- pytorch\n",
    "- numpy\n",
    "- xarray\n",
    "- feerci\n",
    "- pyannote.audio\n",
    "- pyannote.database\n",
    "- pyannote.metrics\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from xarray import DataArray\n",
    "from feerci import feerci\n",
    "from pyannote.core.utils.distance import cdist\n",
    "from pyannote.audio.features import Pretrained\n",
    "from pyannote.audio.features.utils import get_audio_duration\n",
    "from pyannote.audio.applications.speaker_embedding import SpeakerEmbedding\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "from pyannote.metrics.binary_classification import det_curve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialize the database with the preprocessors needed, and we define some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessors make sure to look for wav files and to calculate the audio duration\n",
    "preprocessors = {'audio': FileFinder(), 'duration': get_audio_duration}\n",
    "\n",
    "# We use the VoxCeleb1_X protocol, with a train and dev set resulting from splitting the original dev\n",
    "protocol = get_protocol('VoxCeleb.SpeakerVerification.VoxCeleb1_X', preprocessors=preprocessors)\n",
    "\n",
    "\n",
    "# A function to get embeddings from a file\n",
    "def get_embedding(file, pretrained, mean=False):\n",
    "    emb = []\n",
    "    for f in file.files():\n",
    "        if 'try_with' in f:\n",
    "            segments = f['try_with']\n",
    "        else:\n",
    "            segments = f['annotation'].get_timeline()\n",
    "        for segment in segments:\n",
    "            emb.append(pretrained(f).crop(segment, mode='center'))\n",
    "    emb = np.vstack(emb)\n",
    "    if mean:\n",
    "        emb = np.mean(emb, axis=0, keepdims=True)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# A function to calculate the EER on a subset of VoxCeleb1_X\n",
    "def run_experiment(distance, subset):\n",
    "    y_pred, y_true = [], []\n",
    "    for trial in tqdm(getattr(protocol, f'{subset}_trial')()):\n",
    "        file1 = trial['file1']\n",
    "        hash1 = get_hash(file1)\n",
    "        file2 = trial['file2']\n",
    "        hash2 = get_hash(file2)\n",
    "        y_pred.append(distance.data[index1[hash1], index2[hash2]])\n",
    "        y_true.append(trial['reference'])\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    eer, ci_lower, ci_upper, _ = feerci(-y_pred[y_true == 0], -y_pred[y_true == 1], is_sorted=False)\n",
    "    return {\n",
    "        'eer': eer,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to load the model. You can do so either by loading the model hosted in this repository or a variant trained on segments of variable duration, which is available as pyannote's default speaker embedding model.\n",
    "\n",
    "If you choose pyannote's model, bear in mind that performance might differ from what's in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model to use with 3s segments extracted with a 100ms sliding window\n",
    "# Make sure to change the device to `cpu` if your machine doesn't have a GPU\n",
    "model = Pretrained(validate_dir='models/AAM/train/VoxCeleb.SpeakerVerification.VoxCeleb2.train'\n",
    "                   '/validate_equal_error_rate/VoxCeleb.SpeakerVerification.VoxCeleb1_X.development',\n",
    "                   duration=3., step=.0333, device='cuda')\n",
    "\n",
    "# Alternatively, you can use a version of this model that we have trained for variable duration segments,\n",
    "# which is available as pyannote's default speaker embedding model\n",
    "# model = torch.hub.load('pyannote/pyannote-audio', 'emb', duration=3., step=.0333, device='cuda')\n",
    "\n",
    "print(f'Embeddings have dimension {model.dimension:d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections we will evaluate this model on `VoxCeleb1.test` and compare results using raw cosine distances as well as after score normalization using adaptive s-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with Raw Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is ready, we can use it for inference. In this section we evaluate it using the raw cosine distance between embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a while depending on your machine, as embeddings are being calculated from raw audio using the pretrained neural network (~10min with an Nvidia GTX1080).\n",
    "\n",
    "If you wish to precalculate embeddings for `VoxCeleb1_X` (ex. to run this faster next time), you should use `pyannote-audio emb apply`. An example can be found [here](https://github.com/pyannote/pyannote-audio/tree/develop/tutorials/models/speaker_embedding#application).\n",
    "\n",
    "If you have precalculated embeddings, make sure to use the `Precomputed` class from `pyannote.audio.features` instead of `Pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hash = lambda file: SpeakerEmbedding.get_hash(file)\n",
    "\n",
    "# hash to embedding mapping\n",
    "cache1 = dict()\n",
    "cache2 = dict()\n",
    "\n",
    "# hash to index mapping\n",
    "index1 = dict()\n",
    "index2 = dict()\n",
    "\n",
    "n_file1 = 0\n",
    "n_file2 = 0\n",
    "\n",
    "# Get embeddings for every trial in Test\n",
    "for trial in tqdm(protocol.test_trial(), total=37720):\n",
    "    \n",
    "    file1 = trial['file1']\n",
    "    hash1 = get_hash(file1)\n",
    "    if hash1 not in cache1:\n",
    "        cache1[hash1] = get_embedding(file1, model, mean=True)\n",
    "        index1[hash1] = n_file1\n",
    "        n_file1 += 1\n",
    "    \n",
    "    file2 = trial['file2']\n",
    "    hash2 = get_hash(file2)\n",
    "    if hash2 not in cache2:\n",
    "        cache2[hash2] = get_embedding(file2, model, mean=True)\n",
    "        index2[hash2] = n_file2\n",
    "        n_file2 += 1\n",
    "\n",
    "hashes1 = list(cache1.keys())\n",
    "hashes2 = list(cache2.keys())\n",
    "emb1 = np.vstack(list(cache1.values()))\n",
    "emb2 = np.vstack(list(cache2.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cosine distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = DataArray(\n",
    "    cdist(emb1, emb2, metric='cosine'),\n",
    "    dims=('file1', 'file2'),\n",
    "    coords=(hashes1, hashes2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the EER on VoxCeleb1 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results = run_experiment(distance, 'test')\n",
    "print(f\"EER with raw distances: {100 * raw_results['eer']:.2f} in \"\n",
    "      f\"[{100 * raw_results['ci_lower']:.2f}, {100 * raw_results['ci_upper']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with Adaptive S-Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we improve the above EER with the score normalization method called Adaptive S-Norm, which consists in:\n",
    "\n",
    "1) Determining a cohort set of embeddings (different from the model's training set, in our case `VoxCeleb2.dev`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cohort embeddings from VoxCeleb1_X.train\n",
    "cohort_embedding = dict()\n",
    "for cohort_file in tqdm(protocol.train(), total=143506):\n",
    "    speaker = cohort_file['annotation'].argmax()\n",
    "    embedding = get_embedding(cohort_file, model, mean=False)\n",
    "    cohort_embedding.setdefault(speaker, []).append(embedding)\n",
    "\n",
    "# The cohort consists of the mean embedding for each speaker\n",
    "cohort_speakers = list(cohort_embedding.keys())\n",
    "cohort = np.vstack([np.mean(np.vstack(cohort_embedding[speaker]), axis=0, keepdims=True) \n",
    "                    for speaker in cohort_speakers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculating the raw score of the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distances between each trial embedding (file1 and file2) and the cohort\n",
    "distance1 = DataArray(\n",
    "    cdist(emb1, cohort, metric='cosine'),\n",
    "    dims=('file1', 'cohort'),\n",
    "    coords=(hashes1, cohort_speakers))\n",
    "\n",
    "distance2 = DataArray(\n",
    "    cdist(emb2, cohort, metric='cosine'),\n",
    "    dims=('file2', 'cohort'),\n",
    "    coords=(hashes2, cohort_speakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Calculating the mean and std of N most similar scores to each embedding in the trials (N=500 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our N\n",
    "COHORT_SIZE = 400\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for all file1\n",
    "data1 = np.partition(distance1.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mz = np.mean(data1, axis=1) \n",
    "sz = np.std(data1, axis=1)\n",
    "mz = DataArray(mz, dims=('file1',), coords=(hashes1,))\n",
    "sz = DataArray(sz, dims=('file1',), coords=(hashes1,))\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for all file2\n",
    "data2 = np.partition(distance2.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mt = np.mean(data2, axis=1) \n",
    "st = np.std(data2, axis=1)\n",
    "mt = DataArray(mt, dims=('file2',), coords=(hashes2,))\n",
    "st = DataArray(st, dims=('file2',), coords=(hashes2,))\n",
    "\n",
    "# Normalize\n",
    "distance_z = (distance - mz) / sz\n",
    "distance_t = (distance - mt) / st\n",
    "distance_s = 0.5 * (distance_z + distance_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Normalizing the trial's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the DET curve on test and print the EER value\n",
    "ada_snorm_results = run_experiment(distance_s, 'test')\n",
    "print(f\"EER with Adaptive S-Norm: {100 * ada_snorm_results['eer']:.2f} in\"\n",
    "      f\"[{100 * ada_snorm_results['ci_lower']:.2f}, {100 * ada_snorm_results['ci_upper']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! If you have any questions or suggestions, please feel free to open an issue in [pyannote-audio](https://github.com/pyannote/pyannote-audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
